### median_house_value : target (lable/y)
### total_rooms : features (x)


import math

from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
import tensorflow as tf
from tensorflow.python.data import Dataset

tf.logging.set_verbosity(tf.logging.ERROR)
pd.options.display.max_rows = 10
pd.options.display.float_format = '{:.1f}'.format

### Load Data
california_housing_dataframe = pd.read_csv("https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv", sep=",")

"""
 longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \
0     -114.3      34.2                15.0       5612.0          1283.0   
1     -114.5      34.4                19.0       7650.0          1901.0   
2     -114.6      33.7                17.0        720.0           174.0   
3     -114.6      33.6                14.0       1501.0           337.0   
4     -114.6      33.6                20.0       1454.0           326.0   

   population  households  median_income  median_house_value  
0      1015.0       472.0            1.5             66900.0  
1      1129.0       463.0            1.8             80100.0  
2       333.0       117.0            1.7             85700.0  
3       515.0       226.0            3.2             73400.0  
4       624.0       262.0            1.9             65500.0  
"""

### Randomlize Data by random reindexing
california_housing_dataframe = california_housing_dataframe.reindex(np.random.permutation(california_housing_dataframe.index))

### Divid media_house_value by 1000
california_housing_dataframe['median_house_value'] =/ 1000.0

### Define and configure feature_column
    ### Pandas single square bracket returns Series, double square bracket returns Dataframe. 
my_feature = california_housing_dataframe[['total_rooms']]

feature_columns = [tf.feature_column.numeric_column("total_rooms")]

### Define lable / target
targets = california_housing_dataframe["median_house_value"]

### Use gradient descent as the optimizer for training the model
my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimzer, 5.0)

# Configure the linear regression model with feature columns and optmizer 
linear_regressor = tf.estimator.LinearRegressor(feature_colums=feature_columns, optimizer=my_optimizer)

def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):
    """
    Trains a linear regression model of one feature
    
    features: pandas DataFrame of features
    targets: pandas DataFrame of targets
    batch_size: Size of batches to be passed to th model
    shuffle: whether to shuffle the data
    num_epochs: Number of epoches for which data should be repeated. None = repeat indefinitely
    
    Return:
      Tuple o (features, lables) for next data batch
    """
    
    ### Convert pandas data into a dict of np arrays
    features = {key: np.array(value) for key, value in dict(features).items()}
    
    ### {'total_rooms': array([ 555., 2649.,  388., ...,  863., 3678., 4518.])}
    
    # Construct a dataset
    ds = Dataset.from_tensor_slices((features, targets))
    
    # Configure batching / repeating
    ds = ds.batch(batch_size).repeat(num_epochs)
    
    if shuffle:
        ds = ds.shuffle(buffer_size=10000)
    
    # return the next batch of data
    features, label = ds.make_one_shot_iterator().get_next()
    return features, labels
    
# train the model
_ = linear_regressor.train(input_fn = lambda: my_input_fn(my_feature, targets), steps=100)

# Create an input function for predictions
prediction_input_fn = lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)

# Call predict() on the linear_regressor to make predictions
predictions = linear_regressor.predict(input_fn=prediction_input_fn)
### prediction ---> generator 
### list(prediction) --->
### [{'predictions': array([0.02774989], dtype=float32)}, {'predictions': array([0.1324493], dtype=float32)}, ... ]

# Format predictions a a Numpy array
predictions = np.array([item['predictions'][0] for item in predictions])

# MSE and RMSE
mean_squared_error = metrics.mean_squared_error(predictions, targets)
root_mean_squared_error = math.sqrt(mean_squared_error)
print("Mean Squared Error (on training data): %0.3f" % mean_squared_error)
print("Root Mean Squared Error (on training data): %0.3f" % root_mean_squared_error)

min_house_value = california_housing_dataframe["median_house_value"].min()
max_house_value = california_housing_dataframe["median_house_value"].max()
min_max_difference = max_house_value - min_house_value

print("Min. Median House Value: %0.3f" % min_house_value)
print("Max. Median House Value: %0.3f" % max_house_value)
print("Difference between Min. and Max.: %0.3f" % min_max_difference)
print("Root Mean Squared Error: %0.3f" % root_mean_squared_error)

sample = california_housing_dataframe.sample(n=300)
# Get the min and max total_rooms values.
x_0 = sample["total_rooms"].min()
x_1 = sample["total_rooms"].max()

# Get the list of names in the linear_regreesor tf.estimator
names = linear_regressor.get_variable_names()
### names = ['global_step', 'linear/linear_model/bias_weights', 'linear/linear_model/total_rooms/weights']

# Get final weight and bias generated during training
weight = linear_regressor.get_variable_value('linear/linear_model/total_rooms/weights')[0]
print(weight)
bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')
print(bias)
### weight = [[4.999972e-05]]
### weight[0] = [4.999972e-05]

# Get the predicted median_house_values for the min and max total_rooms values
y_0 = weight * x_0 + bias
y_1 = weight * x_1 + bias

# Plot our regression lin from (x_0, y_0) to (x_1, y_1)
plt.plot([x_0, x_1], [y_0, y_1], c='r')
plt.ylabel("median_house_value")
plt.xlabel("total_rooms")
plt.scatter(sample["total_rooms"], sample["median_house_value"])
plt.show()




### Wrap it into a function

def train_model(learning_rate, steps, batch_size, input_feature="total_rooms")
  """Trains a linear regression model of one feature.
  
    Args:
    learning_rate: A `float`, the learning rate.
    steps: A non-zero `int`, the total number of training steps. A training step
      consists of a forward and backward pass using a single batch.
    batch_size: A non-zero `int`, the batch size.
    input_feature: A `string` specifying a column from `california_housing_dataframe`
      to use as input feature.
  """
  
  period = 10
  steps_per_period = steps / periods
  
  my_feature = input_feature
  my_feature_data = california_housing_dataframe[[my_feature]]
  
  my_label = "median_house_value"
  targets = california_housing_dataframe[my_label]
  
  # Create feature columns
  feature_columns = [tf.feature_column.numeric_column(my_feature)]
  
  # Create input functions for training and prediction
  training_input_fn = lambda: my_input_fn(my_feature_data, targets, batch_size=batch_size)
  prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)
  
  # Create a linear regressor object
  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
  linear_regressor = tf.estimator.LinearRegressor(feature_columns=feature_columns, optimizer=my_optimizer)
  
  # Set up plot of state of model's line each period
  plt.figure(figsize=(15, 6))
  plt.subplot(1, 2, 1)
  plt.title("Learned Line by Period")
  plt.ylabel(my_label)
  plt.xlabel(my_feature)
  sample = california_housing_dataframe.sample(n=300)
  plt.scatter(sample[my_feature], sample[my_label])
  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]

  # Train the model inside a loop for periodically assess loss metrics
  print("Training model...")
  print("RMSE (on training data):")
  root_mean_squared_errors = []
  for period in range(0, periods):
    linear_regressor.train(input_fn=training_input_fn, steps=steps_per_period)
    
    # Compute predictions
    predictions = linear_regressor.predict(input_fn=prediction_input_fn_
    predictions = np.array([item['predictions'][0] for item in predictions])
    
    # Compute loss
    root_mean_squared_error = math.sqrt(metrics.mean_squared_error(predictions, targets))
    print("  period %02d : %0.2f" % (period, root_mean_squared_error))
    root_mean_squared_errors.append(root_mean_squared_error)
    
    # Track weights and biases over time
    y_extents = np.array([0, sample[my_label].max()])
    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]
    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')
    
    x_extents = (y_extents - bias) / weight
    x_extents = np.maximum(np.minimum(x_extents,
                                      sample[my_feature].max()),
                           sample[my_feature].min())
    y_extents = weight * x_extents + bias
    plt.plot(x_extents, y_extents, color=colors[period])     
  print("Model training finished.")
  
  # Output a graph of loss metrics over periods.
  plt.subplot(1, 2, 2)
  plt.ylabel('RMSE')
  plt.xlabel('Periods')
  plt.title("Root Mean Squared Error vs. Periods")
  plt.tight_layout()
  plt.plot(root_mean_squared_errors)

  # Output a table with calibration data.
  calibration_data = pd.Dataframe()
  calibration_data['predictions'] = pd.Series(predictions)
  calibration_data['targets'] = pd.Series(targets)
  display.display(calibration_data.describe())
  
  print("Final RMSE (on training data): %0.2f" % root_mean_squared_error)


